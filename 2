#!/usr/bin/env python3
"""
Анализ данных продаж с использованием Hive
Задача: найти топ-10 товаров по количеству продаж
"""
import subprocess
import os
import sys
import tempfile

class SalesAnalysis:
    def __init__(self):
        self.local_file = '/opt/data/myfile.csv'
        self.hdfs_output_dir = '/user/hadoop/output/'
        self.hdfs_file_path = f'{self.hdfs_output_dir}myfile.csv'
        
    def upload_data_to_hdfs(self):
        """Загрузить данные в HDFS"""
        print("Загрузка данных в HDFS")
        
        if not os.path.exists(self.local_file):
            print(f"Файл {self.local_file} не найден")
            return False
        
        print(f"Локальный файл найден: {self.local_file}")
        
        try:
            # Загружаем файл в существующую директорию
            subprocess.run(['hdfs', 'dfs', '-put', '-f', self.local_file, self.hdfs_file_path], check=True)
            print(f"Файл загружен в HDFS: {self.hdfs_file_path}")
            
            # Проверяем размер файла
            size_result = subprocess.run(
                ['hdfs', 'dfs', '-du', '-h', self.hdfs_file_path],
                capture_output=True,
                text=True,
                check=True
            )
            print(f"Размер файла в HDFS: {size_result.stdout.strip()}")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"Ошибка при работе с HDFS: {e}")
            return False

    def run_hive_analysis(self):
        """Выполнить анализ через Hive"""
        print("\nЗапуск анализа через Hive")
        
        # Создаем HQL запрос для поиска топ-10 товаров
        hql_content = """
-- Создание внешней таблицы на основе CSV файла
CREATE EXTERNAL TABLE IF NOT EXISTS sales_data (
    InvoiceNo STRING,
    StockCode STRING,
    Description STRING,
    Quantity INT,
    InvoiceDate STRING,
    UnitPrice DOUBLE,
    CustomerID STRING,
    Country STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hadoop/output/'
TBLPROPERTIES ("skip.header.line.count"="1");

-- Поиск топ-10 товаров по количеству продаж
SELECT 
    StockCode,
    Description,
    SUM(Quantity) AS TotalQuantity,
    COUNT(*) AS TransactionCount,
    ROUND(AVG(UnitPrice), 2) AS AvgUnitPrice,
    ROUND(SUM(Quantity * UnitPrice), 2) AS TotalRevenue
FROM sales_data
WHERE 
    NOT STARTSWITH(InvoiceNo, 'C')  -- Исключаем отмененные заказы
    AND Quantity > 0                 -- Исключаем возвраты
    AND Description IS NOT NULL      -- Исключаем товары без описания
GROUP BY StockCode, Description
ORDER BY TotalQuantity DESC
LIMIT 10;
"""
        
        try:
            # Создаем временный файл с HQL запросом
            with tempfile.NamedTemporaryFile(mode='w', suffix='.hql', delete=False) as f:
                f.write(hql_content)
                temp_file = f.name
            
            print("Создан HQL файл для анализа")
            
            # Запускаем Hive с нашим запросом
            print("Выполнение Hive запроса...")
            result = subprocess.run(
                ['hive', '-f', temp_file],
                capture_output=True,
                text=True
            )
            
            # Удаляем временный файл
            os.unlink(temp_file)
            
            if result.returncode == 0:
                print("Hive анализ выполнен успешно")
                
                # Выводим результаты
                print("\n" + "="*80)
                print("ТОП-10 ТОВАРОВ ПО КОЛИЧЕСТВУ ПРОДАЖ")
                print("="*80)
                
                # Парсим вывод Hive чтобы показать красивые результаты
                lines = result.stdout.split('\n')
                header_printed = False
                
                for line in lines:
                    line = line.strip()
                    # Пропускаем служебные строки
                    if not line or line.startswith('OK') or line.startswith('Time taken') or line.startswith('Logging'):
                        continue
                    
                    # Находим заголовок с названиями колонок
                    if 'StockCode' in line and 'Description' in line and 'TotalQuantity' in line:
                        print("\nКодТовара | Описание | ВсегоПродано | Транзакций | СрЦена | Выручка")
                        print("-" * 90)
                        header_printed = True
                        continue
                    
                    # Выводим строки с данными (предполагаем формат вывода Hive)
                    if header_printed and line and not line.startswith('-'):
                        # Простой вывод строки как есть
                        print(line)
                
                return True
            else:
                print(f"Ошибка выполнения Hive: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"Ошибка при выполнении Hive анализа: {e}")
            return False

    def save_hive_results(self):
        """Сохранить результаты Hive анализа в HDFS"""
        print("\nСохранение результатов в HDFS")
        
        hql_save_content = """
-- Создание таблицы для результатов
CREATE TABLE IF NOT EXISTS top_10_products (
    StockCode STRING,
    Description STRING,
    TotalQuantity INT,
    TransactionCount INT,
    AvgUnitPrice DOUBLE,
    TotalRevenue DOUBLE
)
STORED AS ORC;

-- Вставка результатов
INSERT OVERWRITE TABLE top_10_products
SELECT 
    StockCode,
    Description,
    SUM(Quantity) AS TotalQuantity,
    COUNT(*) AS TransactionCount,
    ROUND(AVG(UnitPrice), 2) AS AvgUnitPrice,
    ROUND(SUM(Quantity * UnitPrice), 2) AS TotalRevenue
FROM sales_data
WHERE 
    NOT STARTSWITH(InvoiceNo, 'C')
    AND Quantity > 0
    AND Description IS NOT NULL
GROUP BY StockCode, Description
ORDER BY TotalQuantity DESC
LIMIT 10;

-- Экспорт в HDFS
INSERT OVERWRITE DIRECTORY '/user/hadoop/output/top_products'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
SELECT * FROM top_10_products;
"""
        
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.hql', delete=False) as f:
                f.write(hql_save_content)
                temp_file = f.name
            
            print("Сохранение результатов...")
            result = subprocess.run(
                ['hive', '-f', temp_file],
                capture_output=True,
                text=True
            )
            
            os.unlink(temp_file)
            
            if result.returncode == 0:
                print("Результаты сохранены в HDFS: /user/hadoop/output/top_products")
                return True
            else:
                print(f"Ошибка сохранения результатов: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"Ошибка при сохранении результатов: {e}")
            return False

    def check_results(self):
        """Проверить результаты в HDFS"""
        print("\nПроверка результатов в HDFS")
        
        try:
            # Проверяем существование результатов
            result = subprocess.run(
                ['hdfs', 'dfs', '-ls', '/user/hadoop/output/top_products/'],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                print("Результаты найдены в HDFS:")
                for line in result.stdout.split('\n'):
                    if 'part-' in line:
                        file_info = line.split()
                        if len(file_info) >= 5:
                            print(f"  Файл: {file_info[-1]}, Размер: {file_info[4]} байт")
                
                # Показываем содержимое результатов
                print("\nСодержимое результатов:")
                cat_result = subprocess.run(
                    ['hdfs', 'dfs', '-cat', '/user/hadoop/output/top_products/000000_0'],
                    capture_output=True,
                    text=True
                )
                
                if cat_result.returncode == 0:
                    lines = cat_result.stdout.strip().split('\n')
                    print("КодТовара,Описание,ВсегоПродано,Транзакций,СрЦена,Выручка")
                    for line in lines:
                        print(line)
                else:
                    print("Не удалось прочитать содержимое файла")
                    
                return True
            else:
                print("Результаты не найдены в HDFS")
                return False
                
        except Exception as e:
            print(f"Ошибка при проверке результатов: {e}")
            return False

    def run_complete_analysis(self):
        """Запустить полный анализ"""
        print("=" * 60)
        print("АНАЛИЗ ПРОДАЖ: ТОП-10 ТОВАРОВ ПО КОЛИЧЕСТВУ ПРОДАЖ")
        print("=" * 60)
        
        # 1. Загружаем данные в HDFS
        if not self.upload_data_to_hdfs():
            return False
        
        # 2. Выполняем Hive анализ
        if not self.run_hive_analysis():
            return False
        
        # 3. Сохраняем результаты в HDFS
        if not self.save_hive_results():
            return False
        
        # 4. Проверяем результаты
        if not self.check_results():
            return False
        
        print("\nАнализ завершен успешно!")
        print("Топ-10 товаров по количеству продаж найден и сохранен в HDFS")
        
        return True

def main():
    analyzer = SalesAnalysis()
    
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\nЗадача выполнена успешно!")
        sys.exit(0)
    else:
        print("\nЗадача завершена с ошибками")
        sys.exit(1)

if __name__ == '__main__':
    main()
