def upload_to_hdfs(local_path, hdfs_path):
    """Загрузить файл в HDFS"""
    import subprocess
    try:
        subprocess.run(['hdfs', 'dfs', '-put', '-f', local_path, hdfs_path], check=True)
        print(f"Файл загружен в HDFS: {hdfs_path}")
        return True
    except Exception as e:
        print(f"Ошибка загрузки в HDFS: {e}")
        return False

# В main перед загрузкой данных:
hdfs_path = "hdfs://hadoop:9000/user/hadoop/myfile.csv"
if upload_to_hdfs(data_file, hdfs_path):
    df = spark.read.csv(hdfs_path, header=True, inferSchema=True)
else:
    # Использовать локальный файл как запасной вариант
    df = load_data(spark, data_file)
